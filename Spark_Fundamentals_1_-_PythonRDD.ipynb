{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals 1 - Introduction to Spark </h1>\n",
    "<h2 align = \"center\"> Python - Working with RDD operations </h2>\n",
    "<h4 align = \"center\"> January 11, 2016 </h4>\n",
    "<br align = \"left\">\n",
    "\n",
    "**Related free online courses:**  \n",
    "- [Spark Fundamentals II](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals-ii/)  \n",
    "- [Data Analysis using R](https://bigdatauniversity.com/bdu-wp/bdu-course/introduction-to-data-analysis-using-r/)  \n",
    "- [Big Data Fundamentals](http://bigdatauniversity.com/bdu-wp/bdu-course/big-data-fundamentals/)  \n",
    "\n",
    "<img src = \"http://spark.apache.org/images/spark-logo.png\", height = 100, align = 'left'>\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg\", height = 95, align = 'left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyzing a log file\n",
    "\n",
    "First, create an RDD by loading in a log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logFile = sc.textFile(\"/resources/LabData/notebook.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color: red\">YOUR TURN:</span> \n",
    "\n",
    "#### In the cell below, Filter out the lines that contains INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.filter(lambda line: \"INFO\" in line).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "13438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Count the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.count()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Count the lines with \"spark\" in it by combining transformation and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2238"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.filter(lambda line: \"spark\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.filter(lambda line: \"spark\" in line).count()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Fetch those lines as an array of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/14 14:29:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53333]',\n",
       " u\"15/10/14 14:29:23 INFO Utils: Successfully started service 'sparkDriver' on port 53333.\",\n",
       " u'15/10/14 14:29:23 INFO DiskBlockManager: Created local directory at /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/blockmgr-c142f2f1-ebb6-4612-945b-0a67d156230a',\n",
       " u'15/10/14 14:29:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fe150378-7bad-42b6-876b-d14e2c193eb6/httpd-ed3f4ab0-7218-48bc-9d8a-3981b1cfe574',\n",
       " u\"15/10/14 14:29:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35726.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/15 15:33:42 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:47412]',\n",
       " u\"15/10/15 15:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 47412.\",\n",
       " u'15/10/15 15:33:42 INFO DiskBlockManager: Created local directory at /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261',\n",
       " u'15/10/15 15:33:42 INFO HttpFileServer: HTTP File server directory is /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/httpd-80730048-1dcb-4da2-8458-8bf3eba96046',\n",
       " u\"15/10/15 15:33:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47915.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/16 13:08:23 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58378]',\n",
       " u\"15/10/16 13:08:23 INFO Utils: Successfully started service 'sparkDriver' on port 58378.\",\n",
       " u'15/10/16 13:08:23 INFO DiskBlockManager: Created local directory at /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0',\n",
       " u'15/10/16 13:08:23 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/httpd-98632027-ee06-401b-a027-b7973b158023',\n",
       " u\"15/10/16 13:08:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34420.\",\n",
       " u'15/10/16 13:13:22 INFO Utils: path = /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/blockmgr-24ee1e5a-9311-4665-8ce7-56fc1f0601a0, already present as root for deletion.',\n",
       " u'15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70/pyspark-2107622a-f8ad-4b5a-b456-f0e414fbed40',\n",
       " u'15/10/16 13:13:22 INFO Utils: Deleting directory /tmp/spark-ca471293-a9b6-4761-b0c9-00799500af70',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/16 13:13:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:38668]',\n",
       " u\"15/10/16 13:13:27 INFO Utils: Successfully started service 'sparkDriver' on port 38668.\",\n",
       " u'15/10/16 13:13:27 INFO DiskBlockManager: Created local directory at /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/blockmgr-e0992345-e860-44ea-aaca-50e75bd99684',\n",
       " u'15/10/16 13:13:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-52bdb6b1-7781-4abd-9758-bfc0d2a578ec/httpd-fe5e1d28-9663-460b-97fd-e2374b912583',\n",
       " u\"15/10/16 13:13:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44407.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/16 14:52:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43750]',\n",
       " u\"15/10/16 14:52:20 INFO Utils: Successfully started service 'sparkDriver' on port 43750.\",\n",
       " u'15/10/16 14:52:20 INFO DiskBlockManager: Created local directory at /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221',\n",
       " u'15/10/16 14:52:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/httpd-a9ac31c5-fdd1-4437-a29f-771847924c71',\n",
       " u\"15/10/16 14:52:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54796.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:09:21 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:43928]',\n",
       " u\"15/10/21 06:09:21 INFO Utils: Successfully started service 'sparkDriver' on port 43928.\",\n",
       " u'15/10/21 06:09:21 INFO DiskBlockManager: Created local directory at /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016',\n",
       " u'15/10/21 06:09:21 INFO HttpFileServer: HTTP File server directory is /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/httpd-3f3cd3ee-81f2-4ba5-be62-ccdb6d62cf52',\n",
       " u\"15/10/21 06:09:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34705.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:18:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34767]',\n",
       " u\"15/10/21 06:18:20 INFO Utils: Successfully started service 'sparkDriver' on port 34767.\",\n",
       " u'15/10/21 06:18:20 INFO DiskBlockManager: Created local directory at /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555',\n",
       " u'15/10/21 06:18:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/httpd-81687cf4-f5a6-4a97-8e52-a6096ad60235',\n",
       " u\"15/10/21 06:18:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58989.\",\n",
       " u'15/10/21 06:44:27 INFO Utils: path = /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46/blockmgr-aad4e583-6a6c-479a-b021-a7e0390ea261, already present as root for deletion.',\n",
       " u'15/10/21 06:44:27 INFO Utils: Deleting directory /tmp/spark-fc035223-3b43-43d1-8d7d-a22dda6b0d46',\n",
       " u'15/10/21 06:44:41 INFO Utils: path = /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a/blockmgr-ed90f7d7-7049-471a-8560-950825742016, already present as root for deletion.',\n",
       " u'15/10/21 06:44:42 INFO Utils: Deleting directory /tmp/spark-228429ff-23c9-4471-88ae-9c5c26535d7a',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:46:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44681]',\n",
       " u\"15/10/21 06:46:03 INFO Utils: Successfully started service 'sparkDriver' on port 44681.\",\n",
       " u'15/10/21 06:46:03 INFO DiskBlockManager: Created local directory at /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62',\n",
       " u'15/10/21 06:46:04 INFO HttpFileServer: HTTP File server directory is /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/httpd-ca2b9527-9689-44df-90b9-94eb76bf22c8',\n",
       " u\"15/10/21 06:46:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33807.\",\n",
       " u'15/10/21 06:46:06 INFO Utils: path = /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5/blockmgr-719b62d4-5020-486a-bf06-ff030c696f62, already present as root for deletion.',\n",
       " u'15/10/21 06:46:06 INFO Utils: Deleting directory /tmp/spark-03b183d8-7092-4253-9183-5738cd14ccf5',\n",
       " u'15/10/21 06:46:18 INFO Utils: path = /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350/blockmgr-5abb109b-5c36-4d13-ac93-7ad13e807555, already present as root for deletion.',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:46:19 INFO Utils: Deleting directory /tmp/spark-2673eda4-a8e0-485b-a32e-b50c3f74e350',\n",
       " u'15/10/21 06:46:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34749]',\n",
       " u\"15/10/21 06:46:20 INFO Utils: Successfully started service 'sparkDriver' on port 34749.\",\n",
       " u'15/10/21 06:46:20 INFO DiskBlockManager: Created local directory at /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f',\n",
       " u'15/10/21 06:46:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/httpd-72d3e35d-a6b4-427b-b7cd-7f40b45041ae',\n",
       " u\"15/10/21 06:46:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43363.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:51:44 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:58291]',\n",
       " u\"15/10/21 06:51:44 INFO Utils: Successfully started service 'sparkDriver' on port 58291.\",\n",
       " u'15/10/21 06:51:44 INFO DiskBlockManager: Created local directory at /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115',\n",
       " u'15/10/21 06:51:44 INFO HttpFileServer: HTTP File server directory is /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/httpd-c3af5d8f-93b1-4ea1-b4cd-d939821a87ee',\n",
       " u\"15/10/21 06:51:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60134.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:53:15 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:52949]',\n",
       " u\"15/10/21 06:53:15 INFO Utils: Successfully started service 'sparkDriver' on port 52949.\",\n",
       " u'15/10/21 06:53:15 INFO DiskBlockManager: Created local directory at /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/blockmgr-33399bc4-6281-42c6-b023-b7bcd4a56bc2',\n",
       " u'15/10/21 06:53:15 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a0d30f27-58f2-4803-b7ce-2f437dce18c1/httpd-0637bf42-85e3-45a9-a395-9fadcb6744a4',\n",
       " u\"15/10/21 06:53:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42148.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:53:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:48625]',\n",
       " u\"15/10/21 06:53:37 INFO Utils: Successfully started service 'sparkDriver' on port 48625.\",\n",
       " u'15/10/21 06:53:37 INFO DiskBlockManager: Created local directory at /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a',\n",
       " u'15/10/21 06:53:37 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/httpd-034f2b34-e002-40b1-9500-9409076170ec',\n",
       " u\"15/10/21 06:53:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53086.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:54:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:34793]',\n",
       " u\"15/10/21 06:54:55 INFO Utils: Successfully started service 'sparkDriver' on port 34793.\",\n",
       " u'15/10/21 06:54:55 INFO DiskBlockManager: Created local directory at /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654',\n",
       " u'15/10/21 06:54:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/httpd-7dd197ab-d78a-45df-a99f-0cdd16edd456',\n",
       " u\"15/10/21 06:54:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50977.\",\n",
       " u'15/10/21 06:54:59 INFO Utils: path = /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb/blockmgr-c7cde9d3-876b-4def-8d6b-103aab7c5654, already present as root for deletion.',\n",
       " u'15/10/21 06:55:00 INFO Utils: Deleting directory /tmp/spark-3bf9d32b-7ee3-487a-b956-ad88489183bb',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 06:55:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:53265]',\n",
       " u\"15/10/21 06:55:20 INFO Utils: Successfully started service 'sparkDriver' on port 53265.\",\n",
       " u'15/10/21 06:55:20 INFO DiskBlockManager: Created local directory at /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e',\n",
       " u'15/10/21 06:55:20 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/httpd-a749eee4-3bc1-429a-94d0-d221f2f3738a',\n",
       " u\"15/10/21 06:55:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45478.\",\n",
       " u'15/10/21 06:55:22 INFO Utils: path = /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/blockmgr-80f6b3de-e725-45a4-8df9-34bf5a8b291e, already present as root for deletion.',\n",
       " u'15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a/pyspark-6db0c8fd-094a-4f08-bd68-dff219e65350',\n",
       " u'15/10/21 06:55:23 INFO Utils: Deleting directory /tmp/spark-a5859c75-a576-49b2-a2a0-a673e1a2738a',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 07:14:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:45827]',\n",
       " u\"15/10/21 07:14:56 INFO Utils: Successfully started service 'sparkDriver' on port 45827.\",\n",
       " u'15/10/21 07:14:56 INFO DiskBlockManager: Created local directory at /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/blockmgr-573c0859-eb51-466c-99b6-84c3311f512c',\n",
       " u'15/10/21 07:14:56 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0f1efbbf-1336-4e53-b9e1-c3a0b791b808/httpd-4a3e389d-7784-4587-95d4-46cd1d001fca',\n",
       " u\"15/10/21 07:14:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53959.\",\n",
       " u'[I 07:56:23.149 NotebookApp] Request start_kernel: kernel_id=None, kernel_name=spark, path=\"\", **kwargs={}',\n",
       " u'[I 07:56:23.151 NotebookApp] Provisioning local kernel: spark',\n",
       " u'15/10/21 07:56:30 [INFO] Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39281]',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@6391b9c: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@2de2a1ad: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@5ceaf435: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@28b59438: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@7856b513: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@51e5a36c: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@65d53974: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'15/10/21 07:56:31 [WARN] o.s.j.u.c.AbstractLifeCycle - FAILED org.spark-project.jetty.server.Server@44069e8f: java.net.BindException: Address already in use',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)',\n",
       " u'\\tat org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)',\n",
       " u'\\tat org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.spark-project.jetty.server.Server.doStart(Server.java:293)',\n",
       " u'\\tat org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:228)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1991)',\n",
       " u'\\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1982)',\n",
       " u'\\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:238)',\n",
       " u'\\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:117)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:448)',\n",
       " u'\\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:448)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply$mcV$sp(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$$anonfun$reallyInitializeSparkContext$1.apply(ComponentInitialization.scala:187)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.reallyInitializeSparkContext(ComponentInitialization.scala:186)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.reallyInitializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeSparkContext(ComponentInitialization.scala:164)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeSparkContext(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.layer.StandardComponentInitialization$class.initializeComponents(ComponentInitialization.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$$anon$1.initializeComponents(SparkKernel.scala:34)',\n",
       " u'\\tat com.ibm.spark.boot.KernelBootstrap.initialize(KernelBootstrap.scala:79)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$delayedInit$body.apply(SparkKernel.scala:39)',\n",
       " u'\\tat com.ibm.spark.SparkKernel$.main(SparkKernel.scala:23)',\n",
       " u'\\tat com.ibm.spark.SparkKernel.main(SparkKernel.scala)',\n",
       " u'res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@176734ca',\n",
       " u'              ?? spark',\n",
       " u'readme: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at textFile at <console>:14',\n",
       " u'\\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)',\n",
       " u'\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)',\n",
       " u'\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)',\n",
       " u'\\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)',\n",
       " u'\\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)',\n",
       " u'\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)',\n",
       " u'\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)',\n",
       " u'\\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)',\n",
       " u'\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1781)',\n",
       " u'\\tat org.apache.spark.rdd.RDD.count(RDD.scala:1099)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)',\n",
       " u'       https://raw.githubusercontent.com/apache/spark/master/README.md',\n",
       " u'readme: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at <console>:14',\n",
       " u'linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:16',\n",
       " u'res11: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:16',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 15:43:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:39173]',\n",
       " u\"15/10/21 15:43:57 INFO Utils: Successfully started service 'sparkDriver' on port 39173.\",\n",
       " u'15/10/21 15:43:57 INFO DiskBlockManager: Created local directory at /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6',\n",
       " u'15/10/21 15:43:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/httpd-4b254c9b-ffb8-4603-bf57-49661e22248d',\n",
       " u\"15/10/21 15:43:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54908.\",\n",
       " u'wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:17',\n",
       " u'res15: Array[(String, Int)] = Array((package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), ([\"Specifying,1), (\"yarn\",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (<http://spark.apache.org/>,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala>,1), (DataFrames,,1), ...[W 16:00:01.901 NotebookApp] zmq message arrived on closed channel',\n",
       " u'a: Array[(String, Int)] = Array((package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), ([\"Specifying,1), (\"yarn\",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (<http://spark.apache.org/>,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala>,1), (DataFrames,,1), (pro...[W 16:00:23.497 NotebookApp] zmq message arrived on closed channel',\n",
       " u'res16: Array[(String, Int)] = Array((package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), ([\"Specifying,1), (\"yarn\",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (<http://spark.apache.org/>,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala>,1), (DataFrames,,1), ...[W 16:00:26.429 NotebookApp] zmq message arrived on closed channel',\n",
       " u'res18: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:17',\n",
       " u'res19: Array[(String, Int)] = Array((package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), ([\"Specifying,1), (\"yarn\",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (<http://spark.apache.org/>,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala>,1), (DataFrames,,1), ...[W 16:01:16.585 NotebookApp] zmq message arrived on closed channel',\n",
       " u'15/10/21 17:02:06 INFO Utils: path = /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8/blockmgr-586c41e5-996a-4a68-87a5-5b736a9618b6, already present as root for deletion.',\n",
       " u'15/10/21 17:02:07 INFO Utils: Deleting directory /tmp/spark-6e54b9ef-eeee-4cb9-bdd3-c91f0bfba6e8',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/21 17:02:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:42749]',\n",
       " u\"15/10/21 17:02:12 INFO Utils: Successfully started service 'sparkDriver' on port 42749.\",\n",
       " u'15/10/21 17:02:12 INFO DiskBlockManager: Created local directory at /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572',\n",
       " u'15/10/21 17:02:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/httpd-fc7a9c70-76bc-4605-958f-e115bd3e8d47',\n",
       " u\"15/10/21 17:02:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45162.\",\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/22 02:32:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:44439]',\n",
       " u\"15/10/22 02:32:26 INFO Utils: Successfully started service 'sparkDriver' on port 44439.\",\n",
       " u'15/10/22 02:32:26 INFO DiskBlockManager: Created local directory at /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc',\n",
       " u'15/10/22 02:32:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/httpd-af60c2aa-69b8-4878-86e3-43b8fccdb6ac',\n",
       " u\"15/10/22 02:32:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59251.\",\n",
       " u'logFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at textFile at <console>:15',\n",
       " u'info: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at filter at <console>:17',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'res45: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at filter at <console>:20',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'a: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at filter at <console>:19',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'              info.filter(line => line.contains(\"spark\")).collect()\\ufffc',\n",
       " u'readmeFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at textFile at <console>:15',\n",
       " u'pom: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[22] at textFile at <console>:15',\n",
       " u'readmeCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[30] at reduceByKey at <console>:17',\n",
       " u'pomCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[33] at reduceByKey at <console>:17',\n",
       " u'res64: Array[(String, Int)] = Array((package,1), (this,1), (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), ([\"Specifying,1), (\"yarn\",1), (page](http://spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (<http://spark.apache.org/>,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala>,1), (DataFrames,,1), ...[W 02:50:17.990 NotebookApp] zmq message arrived on closed channel',\n",
       " u'joined: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[36] at join at <console>:23',\n",
       " u'joined: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[39] at join at <console>:23',\n",
       " u'joinedSum: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[40] at map at <console>:25',\n",
       " u'broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(52)',\n",
       " u'accum: org.apache.spark.Accumulator[Int] = 0',\n",
       " u'15/10/22 04:36:32 INFO Utils: path = /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500/blockmgr-264e8036-5ed0-4b03-b896-6ff04e27f572, already present as root for deletion.',\n",
       " u'15/10/22 04:36:32 INFO Utils: Deleting directory /tmp/spark-dc939ed3-9bec-4751-8e83-df927a97a500',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/22 04:36:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:57259]',\n",
       " u\"15/10/22 04:36:37 INFO Utils: Successfully started service 'sparkDriver' on port 57259.\",\n",
       " u'15/10/22 04:36:37 INFO DiskBlockManager: Created local directory at /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/blockmgr-f28c1757-42ed-4495-bca7-f4693f2f1846',\n",
       " u'15/10/22 04:36:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6ffd920e-2dd5-43d4-a2b8-6b3c3a1ae0c7/httpd-8bda8744-f6f5-481c-9dd9-065b7bf0f7b9',\n",
       " u\"15/10/22 04:36:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34923.\",\n",
       " u'taxi: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[43] at textFile at <console>:15',\n",
       " u'res112: org.apache.spark.SparkContext = org.apache.spark.SparkContext@176734ca',\n",
       " u'sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@55d47e95',\n",
       " u'<console>:17: error: value createSchemaRDD is not a member of org.apache.spark.sql.SQLContext',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[47] at map at <console>:17',\n",
       " u'<console>:20: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:17: error: value createSchemaRDD is not a member of org.apache.spark.sql.SQLContext',\n",
       " u'<console>:25: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:25: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'res117: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[47] at map at <console>:17',\n",
       " u'<console>:25: error: value toDF is not a member of org.apache.spark.api.java.JavaRDD[Weather]',\n",
       " u'<console>:25: error: value registerTempTable is not a member of org.apache.spark.api.java.JavaRDD[Weather]',\n",
       " u'<console>:25: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value toDf is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[51] at map at <console>:26',\n",
       " u'<console>:29: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'<console>:29: error: value registerTempTable is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:115)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog$$anonfun$1.apply(Catalog.scala:115)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.SimpleCatalog.lookupRelation(Catalog.scala:115)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:222)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:233)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:229)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:212)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:229)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:219)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51)',\n",
       " u'\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51)',\n",
       " u'\\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:933)',\n",
       " u'\\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:933)',\n",
       " u'\\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)',\n",
       " u'\\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)',\n",
       " u'\\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)',\n",
       " u'\\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)',\n",
       " u'<console>:26: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[55] at map at <console>:26',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[59] at map at <console>:26',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[63] at map at <console>:26',\n",
       " u'<console>:29: error: value toDF is not a member of org.apache.spark.rdd.RDD[Weather]',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[67] at map at <console>:29',\n",
       " u'res129: org.apache.spark.sql.DataFrame = [date: string, temp: int, precipitation: double]',\n",
       " u'weather: org.apache.spark.rdd.RDD[Weather] = MapPartitionsRDD[72] at map at <console>:29',\n",
       " u'weather: org.apache.spark.sql.DataFrame = [date: string, temp: int, precipitation: double]',\n",
       " u'hottest_with_precip: org.apache.spark.sql.DataFrame = [date: string, temp: int, precipitation: double]',\n",
       " u'import org.apache.spark.mllib.clustering.KMeans',\n",
       " u'import org.apache.spark.mllib.linalg.Vectors',\n",
       " u'taxiFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[90] at textFile at <console>:29',\n",
       " u'taxiData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[93] at filter at <console>:31',\n",
       " u'taxiData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[96] at filter at <console>:31',\n",
       " u'taxiFence: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[100] at filter at <console>:36',\n",
       " u'taxiFence: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[101] at filter at <console>:33',\n",
       " u'taxiFence: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[105] at filter at <console>:36',\n",
       " u'taxi: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[106] at map at <console>:36',\n",
       " u'model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@573d591d',\n",
       " u'15/10/22 05:42:05 INFO Utils: path = /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4/blockmgr-73dbe021-6e2b-43f9-9547-72004cf3a221, already present as root for deletion.',\n",
       " u'15/10/22 05:42:06 INFO Utils: Deleting directory /tmp/spark-682ea8cc-d28a-4715-be6e-9c2bdc684ba4',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/22 05:42:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:32997]',\n",
       " u\"15/10/22 05:42:59 INFO Utils: Successfully started service 'sparkDriver' on port 32997.\",\n",
       " u'15/10/22 05:42:59 INFO DiskBlockManager: Created local directory at /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff',\n",
       " u'15/10/22 05:43:00 INFO HttpFileServer: HTTP File server directory is /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/httpd-5a4d1c53-dd5f-4d13-8f82-fb56ecc67896',\n",
       " u\"15/10/22 05:43:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39140.\",\n",
       " u'15/10/22 05:44:02 INFO Utils: path = /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59/blockmgr-6da140df-5530-460e-a154-b780fb3839ff, already present as root for deletion.',\n",
       " u'15/10/22 05:44:03 INFO Utils: Deleting directory /tmp/spark-cf71c18f-eb1b-4344-8525-a2574c650a59',\n",
       " u'15/10/22 05:44:23 INFO Utils: path = /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3/blockmgr-822dc396-71cd-4fe2-893d-9f536687422a, already present as root for deletion.',\n",
       " u'15/10/22 05:44:24 INFO Utils: Deleting directory /tmp/spark-b42d68f2-0d81-418e-bccc-425aa078bfd3',\n",
       " u'15/10/22 05:44:31 INFO Utils: path = /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/blockmgr-fb8c79d9-cb49-4e14-9eae-02211819594f, already present as root for deletion.',\n",
       " u'15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f/pyspark-9e9ff3f6-2e32-4570-ae7a-22a651278319',\n",
       " u'15/10/22 05:44:32 INFO Utils: Deleting directory /tmp/spark-0f142b40-8af5-41a7-b3fc-b03c2392bf8f',\n",
       " u'15/10/22 05:44:42 INFO Utils: path = /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/blockmgr-ff3b1fac-22e5-4969-8e3a-2aecbf2c0dcc, already present as root for deletion.',\n",
       " u'15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213/pyspark-00ef6c66-4db7-4741-b890-7647fc2d4f76',\n",
       " u'15/10/22 05:44:43 INFO Utils: Deleting directory /tmp/spark-9b53112a-7587-40b2-875d-52372b9f0213',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/22 05:44:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:46937]',\n",
       " u\"15/10/22 05:44:57 INFO Utils: Successfully started service 'sparkDriver' on port 46937.\",\n",
       " u'15/10/22 05:44:57 INFO DiskBlockManager: Created local directory at /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c',\n",
       " u'15/10/22 05:44:57 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/httpd-91230dd5-3f29-4655-906e-228bc7bde472',\n",
       " u\"15/10/22 05:44:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37166.\",\n",
       " u'15/10/22 05:45:01 INFO Utils: path = /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/blockmgr-e7a8bf00-0701-4cb7-a835-b8448d9fe79c, already present as root for deletion.',\n",
       " u'15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f/pyspark-c5a99eda-9137-401b-99a1-ffeae801f695',\n",
       " u'15/10/22 05:45:02 INFO Utils: Deleting directory /tmp/spark-a1ac4ef9-edbb-4d59-84b1-fb6158dd824f',\n",
       " u'15/10/22 05:48:07 INFO Utils: path = /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5/blockmgr-22d08b29-3ede-43e5-b659-7938c320c115, already present as root for deletion.',\n",
       " u'15/10/22 05:48:08 INFO Utils: Deleting directory /tmp/spark-087dfd65-e3af-498d-9d22-e35dd3d35ef5',\n",
       " u'taxi: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[156] at textFile at <console>:29',\n",
       " u'taxiParse: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[157] at map at <console>:31',\n",
       " u'taxiMedKey: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[158] at map at <console>:33',\n",
       " u'taxiMedCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[159] at reduceByKey at <console>:35',\n",
       " u'taxiMedCountsOneLine: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[164] at reduceByKey at <console>:31',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)',\n",
       " u'import org.apache.spark.graphx._',\n",
       " u'users: org.apache.spark.rdd.RDD[(Long, Array[String])] = MapPartitionsRDD[170] at map at <console>:32',\n",
       " u'followerGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@959dc2a',\n",
       " u'graph: org.apache.spark.graphx.Graph[Array[String],Int] = org.apache.spark.graphx.impl.GraphImpl@221c4d1e',\n",
       " u'graph: org.apache.spark.graphx.Graph[Array[String],Int] = org.apache.spark.graphx.impl.GraphImpl@5e79ad5',\n",
       " u'subgraph: org.apache.spark.graphx.Graph[Array[String],Int] = org.apache.spark.graphx.impl.GraphImpl@5ec170a1',\n",
       " u'pagerankGraph: org.apache.spark.graphx.Graph[Double,Double] = org.apache.spark.graphx.impl.GraphImpl@3a95329a',\n",
       " u'userInfoWithPageRank: org.apache.spark.graphx.Graph[(Double, List[String]),Int] = org.apache.spark.graphx.impl.GraphImpl@27ca89fc',\n",
       " u'<console>:45: error: value mkSt is not a member of Array[(org.apache.spark.graphx.VertexId, (Double, List[String]))]',\n",
       " u\"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\",\n",
       " u'15/10/22 06:12:32 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.17.0.22:56255]',\n",
       " u\"15/10/22 06:12:32 INFO Utils: Successfully started service 'sparkDriver' on port 56255.\",\n",
       " u'15/10/22 06:12:32 INFO DiskBlockManager: Created local directory at /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/blockmgr-88b233e4-cd52-4810-b90f-fd20425e41c4',\n",
       " u'15/10/22 06:12:32 INFO HttpFileServer: HTTP File server directory is /tmp/spark-7ca4125c-fd13-4197-aa7f-9fe6163feca2/httpd-0f8a2ab5-bb96-4597-98e9-db0d62952c1f',\n",
       " u\"15/10/22 06:12:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34717.\",\n",
       " u'import org.apache.spark._',\n",
       " u'import org.apache.spark.streaming._',\n",
       " u'import org.apache.spark.streaming.StreamingContext._',\n",
       " u'ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@79fbebf7',\n",
       " u'lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@6788da90',\n",
       " u'pass: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@f747af2',\n",
       " u'\\tat org.apache.spark.streaming.ContextWaiter.waitForStopOrError(ContextWaiter.scala:63)',\n",
       " u'\\tat org.apache.spark.streaming.StreamingContext.awaitTermination(StreamingContext.scala:623)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)',\n",
       " u'\\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)',\n",
       " u'\\tat com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)',\n",
       " u'\\tat com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:73)',\n",
       " u'\\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:59)',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.filter(lambda line: \"spark\" in line).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Highlight text for answer:\n",
    "\n",
    "<textarea rows=\"3\" cols=\"80\" style=\"color: white\">\n",
    "info.filter(lambda line: \"spark\" in line).collect()\n",
    "</textarea>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "View the graph of an RDD using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[5] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      " |  /resources/LabData/notebook.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print logFile.filter(lambda line: \"spark\" in line).toDebugString()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Joining RDDs\n",
    "\n",
    "Next, you are going to create RDDs for the README and the CHANGES file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readmeFile = sc.textFile(\"/resources/LabData/README.md\")\n",
    "pomFile = sc.textFile(\"/resources/LabData/pom.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many Spark keywords are in each file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print readmeFile.filter(lambda line: \"Spark\" in line).count()\n",
    "print pomFile.filter(lambda line: \"Spark\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do a WordCount on each RDD so that the results are (K,V) pairs of (word,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\" \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\" \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the array for either of them, just call the collect function on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme Count\n",
      "\n",
      "[(u'', 67), (u'when', 1), (u'R,', 1), (u'including', 3), (u'computation', 1), (u'using:', 1), (u'guidance', 3), (u'Scala,', 1), (u'environment', 1), (u'only', 1), (u'rich', 1), (u'Apache', 1), (u'sc.parallelize(range(1000)).count()', 1), (u'Building', 1), (u'guide,', 1), (u'return', 2), (u'Please', 3), (u'Try', 1), (u'not', 1), (u'Spark', 14), (u'scala>', 1), (u'Note', 1), (u'cluster.', 1), (u'./bin/pyspark', 1), (u'have', 1), (u'params', 1), (u'through', 1), (u'GraphX', 1), (u'[run', 1), (u'abbreviated', 1), (u'[project', 2), (u'##', 8), (u'library', 1), (u'see', 1), (u'\"local\"', 1), (u'[Apache', 1), (u'will', 1), (u'#', 1), (u'processing,', 1), (u'for', 12), (u'[building', 1), (u'provides', 1), (u'print', 1), (u'supports', 2), (u'built,', 1), (u'[params]`.', 1), (u'available', 1), (u'run', 7), (u'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), (u'This', 2), (u'Hadoop,', 2), (u'Tests', 1), (u'example:', 1), (u'-DskipTests', 1), (u'Maven](http://maven.apache.org/).', 1), (u'programming', 1), (u'running', 1), (u'against', 1), (u'site,', 1), (u'comes', 1), (u'package.', 1), (u'and', 10), (u'package.)', 1), (u'prefer', 1), (u'documentation,', 1), (u'submit', 1), (u'tools', 1), (u'use', 3), (u'from', 1), (u'For', 2), (u'fast', 1), (u'systems.', 1), (u'<http://spark.apache.org/>', 1), (u'Hadoop-supported', 1), (u'way', 1), (u'README', 1), (u'MASTER', 1), (u'engine', 1), (u'building', 3), (u'usage', 1), (u'Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1), (u'instance:', 1), (u'with', 4), (u'protocols', 1), (u'And', 1), (u'this', 1), (u'setup', 1), (u'shell:', 2), (u'project', 1), (u'See', 1), (u'following', 2), (u'distribution', 1), (u'detailed', 2), (u'file', 1), (u'stream', 1), (u'is', 6), (u'higher-level', 1), (u'tests', 2), (u'1000:', 2), (u'sample', 1), (u'[\"Specifying', 1), (u'Alternatively,', 1), (u'./bin/run-example', 2), (u'need', 1), (u'You', 3), (u'instructions.', 1), (u'different', 1), (u'programs,', 1), (u'storage', 1), (u'same', 1), (u'machine', 1), (u'Running', 1), (u'which', 2), (u'you', 4), (u'A', 1), (u'About', 1), (u'sc.parallelize(1', 1), (u'locally.', 1), (u'Hive', 2), (u'optimized', 1), (u'uses', 1), (u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), (u'variable', 1), (u'The', 1), (u'data', 1), (u'a', 10), (u'\"yarn\"', 1), (u'Thriftserver', 1), (u'processing.', 1), (u'./bin/spark-shell', 1), (u'Python', 2), (u'Spark](#building-spark).', 1), (u'clean', 1), (u'the', 21), (u'requires', 1), (u'talk', 1), (u'help', 1), (u'Hadoop', 4), (u'using', 2), (u'high-level', 1), (u'find', 1), (u'web', 1), (u'Shell', 2), (u'how', 2), (u'graph', 1), (u'run:', 1), (u'should', 2), (u'to', 14), (u'module,', 1), (u'given.', 1), (u'directory.', 1), (u'must', 1), (u'do', 2), (u'Programs', 1), (u'Many', 1), (u'YARN,', 1), (u'[\"Third', 1), (u'Example', 1), (u'Once', 1), (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), (u'Because', 1), (u'name', 1), (u'Testing', 1), (u'refer', 2), (u'Streaming', 1), (u'SQL', 2), (u'them,', 1), (u'analysis.', 1), (u'application', 1), (u'set', 2), (u'Scala', 2), (u'thread,', 1), (u'individual', 1), (u'examples', 2), (u'changed', 1), (u'runs.', 1), (u'Pi', 1), (u'More', 1), (u'Python,', 2), (u'Versions', 1), (u'its', 1), (u'version', 1), (u'wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), (u'`./bin/run-example', 1), (u'Configuration', 1), (u'command,', 2), (u'can', 6), (u'core', 1), (u'Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), (u'MASTER=spark://host:7077', 1), (u'Documentation', 1), (u'downloaded', 1), (u'distributions.', 1), (u'Spark.', 1), (u'[\"Building', 1), (u'`examples`', 2), (u'on', 6), (u'works', 1), (u'package', 1), (u'of', 5), (u'APIs', 1), (u'pre-built', 1), (u'Big', 1), (u'or', 3), (u'learning,', 1), (u'locally', 2), (u'overview', 1), (u'one', 2), (u'(You', 1), (u'Online', 1), (u'versions', 1), (u'your', 1), (u'threads.', 1), (u'>>>', 1), (u'SparkPi', 2), (u'contains', 1), (u'system', 1), (u'class', 2), (u'start', 1), (u'build/mvn', 1), (u'basic', 1), (u'configure', 1), (u'that', 3), (u'N', 1), (u'\"local[N]\"', 1), (u'DataFrames,', 1), (u'particular', 3), (u'be', 2), (u'an', 3), (u'easiest', 1), (u'Interactive', 2), (u'cluster', 2), (u'page](http://spark.apache.org/documentation.html)', 1), (u'<class>', 1), (u'example', 3), (u'are', 1), (u'Data.', 1), (u'mesos://', 1), (u'computing', 1), (u'URL,', 1), (u'in', 5), (u'general', 2), (u'To', 2), (u'at', 2), (u'1000).count()', 1), (u'Party', 1), (u'if', 4), (u'built', 1), (u'no', 1), (u'Java,', 1), (u'MLlib', 1), (u'also', 5), (u'other', 1), (u'build', 3), (u'online', 1), (u'several', 1), (u'distribution.', 1), (u'HDFS', 1), (u'[Configuration', 1), (u'spark://', 1), (u'programs', 2), (u'documentation', 3), (u'It', 2), (u'graphs', 1), (u'./dev/run-tests', 1), (u'first', 1), (u'latest', 1)]\n"
     ]
    }
   ],
   "source": [
    "print \"Readme Count\\n\"\n",
    "print readmeCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pom Count\n",
      "\n",
      "[(u'', 2931), (u'agreed', 1), (u'<artifactId>hadoop-mapreduce-client-jobclient</artifactId>', 1), (u'hbase', 1), (u'provided.', 1), (u'copyright', 1), (u'<parent>', 1), (u'distributed', 3), (u'<artifactId>hadoop-core</artifactId>', 1), (u'<artifactId>hadoop-hdfs</artifactId>', 1), (u'</profile>', 6), (u'Apache', 2), (u'CONDITIONS', 1), (u'WARRANTIES', 1), (u'them', 1), (u'<version>1.2.6</version>', 1), (u'<artifactId>spark-hive_${scala.binary.version}</artifactId>', 1), (u'<version>3.2.0</version>', 1), (u'Project', 1), (u'not', 1), (u'writing,', 1), (u'you', 1), (u'Examples</name>', 1), (u'regarding', 1), (u'<version>1.6.0-SNAPSHOT</version>', 1), (u'<artifactId>spark-examples_2.10</artifactId>', 1), (u'...-->', 1), (u'specific', 1), (u'SPARK-4455', 4), (u'<profiles>', 1), (u'</filters>', 1), (u'http://maven.apache.org/xsd/maven-4.0.0.xsd\">', 1), (u'dependencies.', 1), (u'force', 1), (u'<packaging>jar</packaging>', 1), (u'licenses', 1), (u'License', 3), (u'for', 2), (u'<groupId>org.scalacheck</groupId>', 1), (u'<exclude>META-INF/*.SF</exclude>', 1), (u'<id>hadoop-provided</id>', 1), (u'<groupId>commons-codec</groupId>', 1), (u'limitations', 1), (u'we', 1), (u'xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"', 1), (u'<groupId>org.apache.thrift</groupId>', 1), (u'<scope>test</scope>', 2), (u'governing', 1), (u'Profiles', 1), (u'<groupId>org.apache.commons</groupId>', 3), (u'<groupId>commons-cli</groupId>', 1), (u'license', 1), (u'<id>parquet-provided</id>', 1), (u'</transformer>', 2), (u'so', 1), (u'<filter>', 1), (u'present', 1), (u'<groupId>com.google.guava</groupId>', 1), (u'<version>0.9.0</version>', 1), (u'<artifactId>commons-cli</artifactId>', 1), (u'uses', 1), (u'<artifactId>hadoop-annotations</artifactId>', 1), (u'<groupId>org.spark-project.protobuf</groupId>', 1), (u'<skip>true</skip>', 2), (u'<profile>', 6), (u'<groupId>org.apache.cassandra</groupId>', 1), (u'contributor', 1), (u'</filter>', 1), (u'<shadedArtifactAttached>false</shadedArtifactAttached>', 1), (u'<exclude>META-INF/*.RSA</exclude>', 1), (u'use', 1), (u'(ASF)', 1), (u'Unless', 1), (u'BASIS,', 1), (u'<artifactId>spark-bagel_${scala.binary.version}</artifactId>', 1), (u'\"License\");', 1), (u'<groupId>io.netty</groupId>', 2), (u'<url>http://spark.apache.org/</url>', 1), (u'<artifactId>spark-streaming-kafka_${scala.binary.version}</artifactId>', 1), (u'<groupId>org.slf4j</groupId>', 1), (u'but', 1), (u'<groupId>net.jpountz.lz4</groupId>', 1), (u'agreements.', 1), (u'with', 2), (u'</project>', 1), (u'<scope>${hbase.deps.scope}</scope>', 6), (u'<artifactId>avro</artifactId>', 1), (u'<artifactId>hadoop-mapreduce-client-core</artifactId>', 1), (u'language', 1), (u'this', 3), (u'See', 2), (u'following', 1), (u'<artifactId>maven-deploy-plugin</artifactId>', 1), (u'and', 1), (u'NOTICE', 1), (u'want', 1), (u'is', 2), (u'<type>test-jar</type>', 1), (u'file', 3), (u'<artifactId>spark-graphx_${scala.binary.version}</artifactId>', 1), (u'You', 2), (u'<artifactId>hadoop-client</artifactId>', 1), (u'<artifactId>commons-codec</artifactId>', 1), (u'</profiles>', 1), (u'<exclude>META-INF/*.DSA</exclude>', 1), (u'<relativePath>../pom.xml</relativePath>', 1), (u'which', 1), (u'<filters>', 1), (u'<artifactId>jersey-json</artifactId>', 1), (u'Spark', 1), (u'<transformers>', 1), (u'<artifactId>scalacheck_${scala.binary.version}</artifactId>', 1), (u'<id>kinesis-asl</id>', 1), (u'</parent>', 1), (u'may', 2), (u'<artifactId>hbase-protocol</artifactId>', 1), (u'implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">', 1), (u'implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"', 1), (u'The', 2), (u'implied.', 1), (u'a', 1), (u'<artifactId>jruby-complete</artifactId>', 1), (u'<artifactId>scopt_${scala.binary.version}</artifactId>', 1), (u'<groupId>org.apache.hadoop</groupId>', 7), (u'the', 10), (u'</transformers>', 1), (u'<artifactId>spark-streaming_${scala.binary.version}</artifactId>', 1), (u'<artifactSet>', 1), (u'better,', 1), (u'<artifactId>hbase-testing-util</artifactId>', 1), (u'obtain', 1), (u'<include>*:*</include>', 1), (u'<groupId>commons-lang</groupId>', 1), (u'encoding=\"UTF-8\"?>', 1), (u'dependencies', 1), (u'<artifactId>hbase-annotations</artifactId>', 4), (u'Foundation', 1), (u'KIND,', 1), (u'<groupId>com.twitter</groupId>', 1), (u'inclusion', 1), (u'except', 1), (u'<artifactId>hbase-hadoop-compat</artifactId>', 2), (u'to', 5), (u'<artifactId>guava</artifactId>', 1), (u'under', 4), (u'<?xml', 1), (u'Software', 1), (u'<artifactId>jersey-core</artifactId>', 2), (u'<groupId>org.apache.maven.plugins</groupId>', 3), (u'<artifactId>commons-math</artifactId>', 1), (u'express', 1), (u'<resource>reference.conf</resource>', 1), (u'<artifactId>libthrift</artifactId>', 1), (u'<testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>', 1), (u'permissions', 1), (u'<artifactId>hbase-client</artifactId>', 1), (u'</properties>', 6), (u'<artifactId>hbase-server</artifactId>', 1), (u'</includes>', 1), (u'<flume.deps.scope>provided</flume.deps.scope>', 1), (u'<version>${hbase.version}</version>', 7), (u'<artifactId>spark-streaming-zeromq_${scala.binary.version}</artifactId>', 1), (u'<artifactId>jersey-server</artifactId>', 1), (u'<artifactId>spark-streaming-twitter_${scala.binary.version}</artifactId>', 1), (u'<configuration>', 3), (u'OR', 1), (u'<groupId>com.googlecode.concurrentlinkedhashmap</groupId>', 1), (u'(the', 1), (u'applicable', 1), (u'certain', 1), (u'<plugin>', 3), (u'Version', 1), (u'are', 1), (u'<artifactId>spark-parent_2.10</artifactId>', 1), (u'</build>', 1), (u'<outputFile>${project.build.directory}/scala-${scala.binary.version}/spark-examples-${project.version}-hadoop${hadoop.version}.jar</outputFile>', 1), (u'2.0', 1), (u'<artifactId>commons-lang</artifactId>', 1), (u'v2.4,', 1), (u'<transformer', 3), (u'<artifactId>hadoop-auth</artifactId>', 1), (u'<sbt.project.name>examples</sbt.project.name>', 1), (u'required', 1), (u'</dependency>', 25), (u'either', 1), (u'be', 1), (u'<artifactId>commons-math3</artifactId>', 2), (u'<modelVersion>4.0.0</modelVersion>', 1), (u'<exclusions>', 6), (u'</exclusions>', 6), (u'ownership.', 1), (u'License.', 2), (u'by', 1), (u'on', 1), (u'<artifactId>hbase-hadoop1-compat</artifactId>', 1), (u'\"AS', 1), (u'<version>${project.version}</version>', 12), (u'of', 2), (u'compliance', 1), (u'WITHOUT', 1), (u'</excludes>', 1), (u'or', 3), (u'IS\"', 1), (u'<scope>provided</scope>', 8), (u'already', 1), (u'<groupId>com.sun.jersey</groupId>', 4), (u'OF', 1), (u'<hive.deps.scope>provided</hive.deps.scope>', 1), (u'<artifactId>slf4j-api</artifactId>', 1), (u'<groupId>org.scala-lang</groupId>', 1), (u'one', 1), (u'<artifactId>scala-library</artifactId>', 1), (u'<hbase.deps.scope>provided</hbase.deps.scope>', 1), (u'additional', 1), (u'version=\"1.0\"', 1), (u'<groupId>org.apache.cassandra.deps</groupId>', 1), (u'</plugins>', 1), (u'</artifactSet>', 1), (u'<resource>log4j.properties</resource>', 1), (u'ASF', 1), (u'<hadoop.deps.scope>provided</hadoop.deps.scope>', 1), (u'<groupId>com.ning</groupId>', 1), (u'<!--', 8), (u'more', 1), (u'assembly,', 1), (u'<dependency>', 25), (u'<artifactId>concurrentlinkedhashmap-lru</artifactId>', 1), (u'an', 1), (u'<id>flume-provided</id>', 1), (u'<artifact>*:*</artifact>', 1), (u'<outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>', 1), (u'<plugins>', 1), (u'<artifactId>commons-io</artifactId>', 1), (u'xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0', 1), (u'<groupId>jline</groupId>', 1), (u'copy', 1), (u'<exclusion>', 35), (u'<artifactId>netty</artifactId>', 2), (u'<parquet.deps.scope>provided</parquet.deps.scope>', 1), (u'<includes>', 1), (u'work', 1), (u'<id>hive-provided</id>', 1), (u'<groupId>org.apache.spark</groupId>', 14), (u'<artifactId>algebird-core_${scala.binary.version}</artifactId>', 1), (u'<excludes>', 1), (u'<artifactId>protobuf-java</artifactId>', 1), (u'<project', 1), (u'xmlns=\"http://maven.apache.org/POM/4.0.0\"', 1), (u'~', 14), (u'-->', 7), (u'implementation=\"org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer\">', 1), (u'<groupId>org.jruby</groupId>', 1), (u'http://www.apache.org/licenses/LICENSE-2.0', 1), (u'<groupId>org.apache.hbase</groupId>', 12), (u'</configuration>', 3), (u'at', 1), (u'in', 3), (u'<artifactId>spark-core_${scala.binary.version}</artifactId>', 1), (u'Licensed', 1), (u'information', 1), (u'<artifactId>spark-streaming-mqtt_${scala.binary.version}</artifactId>', 1), (u'License,', 1), (u'<groupId>com.github.scopt</groupId>', 1), (u'<artifactId>hbase-common</artifactId>', 1), (u'</plugin>', 3), (u'that', 1), (u'<artifactId>cassandra-all</artifactId>', 1), (u'<artifactId>lz4</artifactId>', 1), (u'<artifactId>maven-install-plugin</artifactId>', 1), (u'<artifactId>jline</artifactId>', 1), (u'</dependencies>', 2), (u'<groupId>commons-io</groupId>', 1), (u'<name>Spark', 1), (u'<groupId>commons-logging</groupId>', 1), (u'<artifactId>compress-lzf</artifactId>', 1), (u'<artifactId>spark-mllib_${scala.binary.version}</artifactId>', 1), (u'<build>', 1), (u'<dependencies>', 2), (u'disable', 1), (u'</exclusion>', 35), (u'law', 1), (u'<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>', 1), (u'<artifactId>commons-logging</artifactId>', 1), (u'<artifactId>spark-streaming-kinesis-asl_${scala.binary.version}</artifactId>', 1), (u'<id>hbase-provided</id>', 1), (u'<properties>', 6), (u'ANY', 1), (u'/>', 1), (u'<artifactId>maven-shade-plugin</artifactId>', 1), (u'software', 1)]\n"
     ]
    }
   ],
   "source": [
    "print \"Pom Count\\n\"\n",
    "print pomCount.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join function combines the two datasets (K,V) and (K,W) together and get (K, (V,W)). Let's join these two counts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = readmeCount.join(pomCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the value to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', (67, 2931)),\n",
       " (u'Apache', (1, 2)),\n",
       " (u'Spark', (14, 1)),\n",
       " (u'this', (1, 3)),\n",
       " (u'for', (12, 2)),\n",
       " (u'use', (3, 1)),\n",
       " (u'uses', (1, 1)),\n",
       " (u'and', (10, 1)),\n",
       " (u'a', (10, 1)),\n",
       " (u'with', (4, 2)),\n",
       " (u'See', (1, 2)),\n",
       " (u'following', (2, 1)),\n",
       " (u'file', (1, 3)),\n",
       " (u'is', (6, 2)),\n",
       " (u'not', (1, 1)),\n",
       " (u'which', (2, 1)),\n",
       " (u'you', (4, 1)),\n",
       " (u'The', (1, 2)),\n",
       " (u'the', (21, 10)),\n",
       " (u'You', (3, 2)),\n",
       " (u'one', (2, 1)),\n",
       " (u'to', (14, 5)),\n",
       " (u'an', (3, 1)),\n",
       " (u'on', (6, 1)),\n",
       " (u'of', (5, 2)),\n",
       " (u'are', (1, 1)),\n",
       " (u'be', (2, 1)),\n",
       " (u'that', (3, 1)),\n",
       " (u'in', (5, 3)),\n",
       " (u'or', (3, 3)),\n",
       " (u'at', (2, 1))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the values together to get the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joinedSum = joined.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if it is correct, print the first five elements from the joined and the joinedSum RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Individial\n",
      "\n",
      "[(u'', (67, 2931)), (u'Apache', (1, 2)), (u'Spark', (14, 1)), (u'this', (1, 3)), (u'for', (12, 2))]\n",
      "\n",
      "\n",
      "Joined Sum\n",
      "\n",
      "[(u'', 2998), (u'Apache', 3), (u'Spark', 15), (u'this', 4), (u'for', 14)]\n"
     ]
    }
   ],
   "source": [
    "print \"Joined Individial\\n\"\n",
    "print joined.take(5)\n",
    "\n",
    "print \"\\n\\nJoined Sum\\n\"\n",
    "print joinedSum.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared variables\n",
    "\n",
    "Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. These variables are copied to each machine, and no updates to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.\n",
    "\n",
    "### Broadcast variables\n",
    "\n",
    "Broadcast variables are useful for when you have a large dataset that you want to use across all the worker nodes. A read-only variable is cached on each machine rather than shipping a copy of it with tasks. Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage.\n",
    "\n",
    "\n",
    "Read more here: [http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables)\n",
    "\n",
    "Create a broadcast variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the value, type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "Accumulators are variables that can only be added through an associative operation. It is used to implement counters and sum efficiently in parallel. Spark natively supports numeric type accumulators and standard mutable collections. Programmers can extend these for new types. Only the driver can read the values of the accumulators. The workers can only invoke it to increment the value.\n",
    "\n",
    "Create the accumulator variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next parallelize an array of four integers and run it through a loop to add each integer value to the accumulator variable. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def f(x):\n",
    "    global accum\n",
    "    accum += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, iterate through each element of the rdd and apply the function f on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the current value of the accumulator variable, type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a value of 10.\n",
    "\n",
    "This command can only be invoked on the driver side. The worker nodes can only increment the accumulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key-value pairs\n",
    "\n",
    "You have already seen a bit about key-value pairs in the Joining RDD section.\n",
    "\n",
    "Create a key-value pair of two characters. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair = ('a', 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the value of the first index use [0] and [1] method for the 2nd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print pair[0]\n",
    "\n",
    "print pair[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lab will show you how to create Spark applications using SQL, Graphx, Mlib, etc. The lab is only available in Scala. \n",
    "\n",
    "<h1 align=\"center\" style=\"font-family: Monaco;\">Continue on \"[Spark Fundamentals 1 - ScalaLibs.ipynb](/api/v1/resources/Spark%20Fundamentals%201%20-%20ScalaLibs.ipynb)\"</h1>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
